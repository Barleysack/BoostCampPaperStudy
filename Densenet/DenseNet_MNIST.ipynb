{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DenseNet_MNIST.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmbqZxqQc-h6"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils import data as D\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import random\n",
        "import torchsummary\n",
        "from torchvision import datasets\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOANFjdNdI8K",
        "outputId": "bd49cc26-cd47-483d-8e9f-bafa58fa68c7"
      },
      "source": [
        "\n",
        "print(torch.__version__)\n",
        "\n",
        "\n",
        "batch_size = 64\n",
        "validation_ratio = 0.1\n",
        "random_seed = 7993\n",
        "initial_lr = 0.01\n",
        "num_epoch = 10\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#==============데이터 적재 파트=====================================================\n",
        "#데이터 어그멘테이션은 스킵합니다. MNIST는 그 자체로 충분할 터.\n",
        "transform_train = transforms.Compose([\n",
        "        #transforms.Resize(32),\n",
        "        \n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.repeat(3,1,1)),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n",
        "\n",
        "transform_validation = transforms.Compose([\n",
        "        #transforms.Resize(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.repeat(3,1,1)),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n",
        "\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "        #transforms.Resize(32),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.repeat(3,1,1)),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n",
        "\n",
        "trainset = datasets.MNIST(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "\n",
        "\n",
        "\n",
        "validset = datasets.MNIST(\n",
        "    root='./data', train=True, download=True, transform=transform_validation)\n",
        "\n",
        "testset = datasets.MNIST(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "num_train = len(trainset)\n",
        "indices = list(range(num_train))\n",
        "split = int(np.floor(validation_ratio * num_train))\n",
        "\n",
        "np.random.seed(random_seed)\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=batch_size, sampler=train_sampler, num_workers=0\n",
        ")\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(\n",
        "    validset, batch_size=batch_size, sampler=valid_sampler, num_workers=0\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=batch_size, shuffle=False, num_workers=0\n",
        ")\n",
        "\n",
        "classes = ('0', '1', '2', '3',\n",
        "           '4', '5', '6', '7', '8', '9')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.9.0+cu102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-G-_S6tdEtc",
        "outputId": "e71af110-db78-41e1-aa59-79bbf29d5ce6"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "#================모듈 파트============================================================================\n",
        "\n",
        "class ConvReluBatch(nn.Module):                     #DenseNet에 이런 뭉태기가 많이 쓰인다고 해서 가져왔습니다. \n",
        "    def __init__(self, nin, nout, kernel_size, stride, padding, bias = False):\n",
        "        super(ConvReluBatch,self).__init__()\n",
        "        self.batch_norm = nn.BatchNorm2d(nin)\n",
        "        self.relu = nn.ReLU(True)\n",
        "        self.conv = nn.Conv2d(nin,nout,kernel_size=kernel_size,stride=stride,padding=padding,bias=bias)\n",
        "    def forward(self,x):\n",
        "        output = self.batch_norm(x)\n",
        "        output = self.relu(output)\n",
        "        output = self.conv(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "class bottleneck_layer(nn.Sequential):\n",
        "    def __init__(self, nin, growth_rate, drop_rate=0.2):    \n",
        "      super(bottleneck_layer, self).__init__()\n",
        "      \n",
        "      self.add_module('conv_1x1', ConvReluBatch(nin=nin, nout=growth_rate*4, kernel_size=1, stride=1, padding=0, bias=False))\n",
        "      self.add_module('conv_3x3', ConvReluBatch(nin=growth_rate*4, nout=growth_rate, kernel_size=3, stride=1, padding=1, bias=False))\n",
        "      \n",
        "      self.drop_rate = drop_rate\n",
        "      \n",
        "    def forward(self, x):\n",
        "      bottleneck_output = super(bottleneck_layer, self).forward(x)\n",
        "      if self.drop_rate > 0:\n",
        "          bottleneck_output = F.dropout(bottleneck_output, p=self.drop_rate, training=self.training)\n",
        "          \n",
        "      bottleneck_output = torch.cat((x, bottleneck_output), 1)\n",
        "      \n",
        "      return bottleneck_output\n",
        "#여기서 torch.cat으로 한번의 bottleneck마다 feature map x가 채널을 따라 누적되는 형태이다. \n",
        "#여기서 1은 차원을 의미하며, 1번은 채널 차원을 의미한다.  (Channel-wise로 연산이 진행된다는 뜻.\n",
        "\n",
        "\n",
        "class TransitionLayer(nn.Sequential): #요걸 사용하면 C, BottleNeck 까지 사용하면 BC. 논문에서는 Compression이라고 칭한다. 하여튼 이번엔 BC모델 구축이다.\n",
        "    def __init__(self,nin,theta=0.5): #theta는 이 Transition Process의 하이퍼파라미터로서, 1x1 conv의 출력 특성맵 수를 조정한다. \n",
        "        super(TransitionLayer,self).__init__()\n",
        "        self.add_module('1X1 Conv',ConvReluBatch(nin=nin,nout=int(nin*theta),kernel_size=1,stride=1,padding=0,bias=False))\n",
        "        self.add_module('2x2 AvgPooling', nn.AvgPool2d(kernel_size=2, stride=2, padding=0))\n",
        "\n",
        "\n",
        "\n",
        "class DenseBlock(nn.Sequential): #단순히 nin과 feature map의 컨트롤용 growth_rate에 따라 bottleneck layer를 쌓아둔 네트워크 형태의 블록.\n",
        "    def __init__(self, nin, num_bottleneck_layers, growth_rate, drop_rate=0.2):\n",
        "        super(DenseBlock, self).__init__()\n",
        "\n",
        "        for i in range(num_bottleneck_layers):\n",
        "              nin_bottleneck_layer = nin + growth_rate * i\n",
        "              self.add_module('BottleneckNo_%d' % i, bottleneck_layer(nin=nin_bottleneck_layer, growth_rate=growth_rate, drop_rate=drop_rate))\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "class DenseNet(nn.Module):\n",
        "    def __init__(self, growth_rate =12 , num_layers=100,theta=0.5,drop_rate=0.2,num_classes=10):\n",
        "        super(DenseNet,self).__init__()\n",
        "        assert ( num_layers - 4)%6 == 0\n",
        "\n",
        "        num_bottleneck_layers=(num_layers-4)//6\n",
        "\n",
        "        self.dense_init = nn.Conv2d(3,growth_rate*2,kernel_size=3,stride=1,padding=1,bias=True)\n",
        "       # 32 x 32 x (growth_rate*2) --> 32 x 32 x [(growth_rate*2) + (growth_rate * num_bottleneck_layers)]\n",
        "        self.dense_block_1 = DenseBlock(nin=growth_rate*2, num_bottleneck_layers=num_bottleneck_layers, growth_rate=growth_rate, drop_rate=drop_rate)\n",
        "\n",
        "        # 32 x 32 x [(growth_rate*2) + (growth_rate * num_bottleneck_layers)] --> 16 x 16 x [(growth_rate*2) + (growth_rate * num_bottleneck_layers)]*theta\n",
        "        nin_transition_layer_1 = (growth_rate*2) + (growth_rate * num_bottleneck_layers) \n",
        "        self.transition_layer_1 = TransitionLayer(nin=nin_transition_layer_1, theta=theta)\n",
        "        \n",
        "        # 16 x 16 x nin_transition_layer_1*theta --> 16 x 16 x [nin_transition_layer_1*theta + (growth_rate * num_bottleneck_layers)]\n",
        "        self.dense_block_2 = DenseBlock(nin=int(nin_transition_layer_1*theta), num_bottleneck_layers=num_bottleneck_layers, growth_rate=growth_rate, drop_rate=drop_rate)\n",
        "\n",
        "        # 16 x 16 x [nin_transition_layer_1*theta + (growth_rate * num_bottleneck_layers)] --> 8 x 8 x [nin_transition_layer_1*theta + (growth_rate * num_bottleneck_layers)]*theta\n",
        "        nin_transition_layer_2 = int(nin_transition_layer_1*theta) + (growth_rate * num_bottleneck_layers) \n",
        "        self.transition_layer_2 = TransitionLayer(nin=nin_transition_layer_2, theta=theta)\n",
        "        \n",
        "        # 8 x 8 x nin_transition_layer_2*theta --> 8 x 8 x [nin_transition_layer_2*theta + (growth_rate * num_bottleneck_layers)]\n",
        "        self.dense_block_3 = DenseBlock(nin=int(nin_transition_layer_2*theta), num_bottleneck_layers=num_bottleneck_layers, growth_rate=growth_rate, drop_rate=drop_rate)\n",
        "        \n",
        "        nin_fc_layer = int(nin_transition_layer_2*theta) + (growth_rate * num_bottleneck_layers) \n",
        "        \n",
        "        # [nin_transition_layer_2*theta + (growth_rate * num_bottleneck_layers)] --> num_classes\n",
        "        self.fc_layer = nn.Linear(nin_fc_layer, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        dense_init_output = self.dense_init(x)\n",
        "        \n",
        "        dense_block_1_output = self.dense_block_1(dense_init_output)\n",
        "        transition_layer_1_output = self.transition_layer_1(dense_block_1_output)\n",
        "        \n",
        "        dense_block_2_output = self.dense_block_2(transition_layer_1_output)\n",
        "        transition_layer_2_output = self.transition_layer_2(dense_block_2_output)\n",
        "        \n",
        "        dense_block_3_output = self.dense_block_3(transition_layer_2_output)\n",
        "        \n",
        "        global_avg_pool_output = F.adaptive_avg_pool2d(dense_block_3_output, (1, 1))                \n",
        "        global_avg_pool_output_flat = global_avg_pool_output.view(global_avg_pool_output.size(0), -1)\n",
        "\n",
        "        output = self.fc_layer(global_avg_pool_output_flat)\n",
        "        \n",
        "        return output\n",
        "\n",
        "#각 DenseBlock 마다 같은 개수의 convolution 연산을 사용한다. \n",
        "\n",
        "\n",
        "def DenseNetBC_100_12():\n",
        "    return DenseNet(growth_rate=12, num_layers=100, theta=0.5, drop_rate=0.2, num_classes=10)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#================================================================================================================\n",
        "net = DenseNetBC_100_12()\n",
        "net.to(device)\n",
        "#torchsummary.summary(net, (3, 32, 32)) #이게 저희 대회때 자주 사용할만 해 보여요\n",
        "\n",
        "    "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DenseNet(\n",
              "  (dense_init): Conv2d(3, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (dense_block_1): DenseBlock(\n",
              "    (BottleneckNo_0): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_1): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(36, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_2): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_3): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_4): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_5): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_6): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_7): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_8): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_9): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_10): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_11): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_12): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_13): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_14): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_15): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (transition_layer_1): TransitionLayer(\n",
              "    (1X1 Conv): ConvReluBatch(\n",
              "      (batch_norm): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv): Conv2d(216, 108, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (2x2 AvgPooling): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "  )\n",
              "  (dense_block_2): DenseBlock(\n",
              "    (BottleneckNo_0): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_1): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_2): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_3): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_4): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_5): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_6): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_7): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_8): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_9): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(216, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_10): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(228, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(228, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_11): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(240, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_12): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(252, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(252, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_13): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(264, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(264, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_14): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(276, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(276, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_15): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (transition_layer_2): TransitionLayer(\n",
              "    (1X1 Conv): ConvReluBatch(\n",
              "      (batch_norm): BatchNorm2d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv): Conv2d(300, 150, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (2x2 AvgPooling): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "  )\n",
              "  (dense_block_3): DenseBlock(\n",
              "    (BottleneckNo_0): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(150, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_1): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(162, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(162, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_2): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(174, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(174, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_3): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(186, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(186, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_4): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(198, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(198, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_5): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(210, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(210, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_6): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(222, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(222, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_7): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(234, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(234, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_8): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(246, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(246, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_9): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(258, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(258, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_10): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(270, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(270, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_11): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(282, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(282, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_12): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(294, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(294, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_13): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(306, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(306, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_14): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(318, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(318, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (BottleneckNo_15): bottleneck_layer(\n",
              "      (conv_1x1): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(330, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): ConvReluBatch(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (fc_layer): Linear(in_features=342, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "mK89zoTndNla",
        "outputId": "8d9d2dfe-aaf0-4f29-d52a-6e4f1730fc57"
      },
      "source": [
        "#===================================훈련 부분=================================\n",
        "Lossfunc = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(),lr=initial_lr,momentum=0.9)\n",
        "LRScheduler= optim.lr_scheduler.MultiStepLR(optimizer=optimizer,milestones=[int(num_epoch * 0.5),\n",
        "             int(num_epoch * 0.75)], gamma=0.1, last_epoch=-1)\n",
        "\n",
        "for epoch in range(num_epoch):  \n",
        "    LRScheduler.step()\n",
        "    \n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss = Lossfunc(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        show_period = 1000\n",
        "        if i % show_period == show_period-1:    # print every \"show_period\" mini-batches\n",
        "            print('[%d, %5d/50000] loss: %.7f' %\n",
        "                  (epoch + 1, (i + 1)*batch_size, running_loss / show_period))\n",
        "            \n",
        "            if running_loss / show_period <=0.02: #일...일종의 Early stopping 추가. 슬쩍 보니까 0.017 이하로는 잘 안내려가더라구요. \n",
        "              break\n",
        "            running_loss = 0.0\n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "print('Finished Training')\n",
        "#====================================평가 부분============\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "                \n",
        "        for i in range(labels.shape[0]):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "            \n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))            \n",
        "            \n",
        "for i in range(10):\n",
        "    print('Accuracy of %5s : %2d %%' % (\n",
        "        classes[i], 100 * class_correct[i] / class_total[i])) \n",
        "    \n",
        "#평가까지 해주는 편안한 셀이 있는데 안쓸 이유가 없죠\n",
        "n_sample = 25\n",
        "sample_indices = np.random.choice(len(testset.targets),n_sample,replace=False)\n",
        "test_x = testset.data[sample_indices]\n",
        "test_y = testset.targets[sample_indices]\n",
        "with torch.no_grad():\n",
        "    net.eval() # to evaluation mode #put model name in\n",
        "    y_pred = net.forward(test_x.view(-1,1,28,28).type(torch.float).to(device)/255.)\n",
        "y_pred = y_pred.argmax(axis=1)\n",
        "plt.figure(figsize=(10,10))\n",
        "for idx in range(n_sample):\n",
        "    plt.subplot(5, 5, idx+1)\n",
        "    plt.imshow(test_x[idx], cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Pred:%d, Label:%d\"%(y_pred[idx],test_y[idx]))\n",
        "plt.show()    \n",
        "print (\"Done\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-8b3dfe3f167a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mshow_period\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6Qt0xCBdXji"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}